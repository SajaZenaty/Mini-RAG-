{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§  Model 3 Section 1: Homework\n",
        "> ## Building a Full RAG Pipeline\n",
        "\n",
        "> **ğŸ¯ Todayâ€™s Goal**: Combine all the parts from Section 1. We will use the **Retriever** (MiniLM) and the **Generator** (DistilBERT) to build a complete, end-to-end Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "---\n",
        "\n",
        "###  recap: The Two Parts of Our RAG System\n",
        "\n",
        "1.  **The Retriever (Part 2)** ğŸ”\n",
        "    * **Model:** `all-MiniLM-L6-v2`\n",
        "    * **Job:** To turn a text query into a vector and use **semantic search** to find the *most relevant* piece of context from our knowledge base.\n",
        "\n",
        "2.  **The Generator (Part 3)** âœï¸\n",
        "    * **Model:** `distilbert-base-cased-distilled-squad`\n",
        "    * **Job:** To take a `question` and a `context` and **extract** the specific answer from within the context.\n",
        "\n",
        "Today, we connect them. The output of the Retriever becomes the input for the Generator.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Bh20Fd7ypN9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  Step 1: Load Models and Knowledge\n",
        "\n",
        "Now, let's load both of our specialized models and define our knowledge base. We have one model for retrieving and one for generating.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vz8_wQuapVMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 2: Load Models (The \"CPU/GPU Split\" Fix)\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# --- 1. Load the Retriever (MiniLM) on the CPU ---\n",
        "# We explicitly tell it to use the 'cpu'.\n",
        "# This is fast enough for a retriever and saves all our VRAM.\n",
        "retriever_model = SentenceTransformer(\n",
        "    'all-MiniLM-L6-v2',\n",
        "    device='cpu'  # Force to CPU\n",
        ")\n",
        "print(f\"âœ… Retriever model (MiniLM) loaded. Using device: cpu\")\n",
        "\n",
        "\n",
        "# --- 2. Load the Generator (DistilBERT) on the GPU ---\n",
        "# We check if a GPU is available and set the device index\n",
        "# 0 = first GPU, -1 = CPU\n",
        "pipeline_device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "generator_model = pipeline(\"question-answering\",\n",
        "                           model=\"distilbert-base-cased-distilled-squad\",\n",
        "                           device=pipeline_device) # Use GPU if available\n",
        "\n",
        "print(f\"âœ… Generator model (DistilBERT) loaded.\")\n",
        "if pipeline_device == 0:\n",
        "    print(\"   -> Running on GPU (Good!)\")\n",
        "else:\n",
        "    print(\"   -> WARNING: Running on CPU (Will be slow, but should work)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp1iXwIQpZn4",
        "outputId": "52db75ee-6852-40c1-84bf-f738f758d3ae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Retriever model (MiniLM) loaded. Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Generator model (DistilBERT) loaded.\n",
            "   -> Running on GPU (Good!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ğŸ“š Step 2: Define and Encode Knowledge Base\n",
        "\n",
        "Here is our simple knowledge base. We will give this \"long-term memory\" to our AI agent.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VYrcM9FrpcAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 3: Define Knowledge Base\n",
        "# This is the \"long-term memory\" of our agent\n",
        "knowledge_base = [\n",
        "    # Original sentences\n",
        "    \"Buddy is a 3-year-old Golden Retriever who loves to play fetch.\",\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"Python is an interpreted, high-level, general-purpose programming language.\",\n",
        "    \"The first person to walk on the Moon was Neil Armstrong in 1969.\",\n",
        "    \"Climate change is the long-term alteration of temperature and typical weather patterns.\",\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "print(f\"ğŸ“š Knowledge base created with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOB-pcM3pedx",
        "outputId": "c8258de2-6ace-4e8a-a227-8826f46a1143"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“š Knowledge base created with 5 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### âœï¸ Task 1: Encode Your Knowledge\n",
        "\n",
        "Your first task is to use the **Retriever model** (`retriever_model`) to encode all the documents in your `knowledge_base`.\n",
        "\n",
        "**Your Goal:** Create a variable called `knowledge_embeddings` that holds the vector representations of all your documents.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rW8H65Pfph0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 4: Task 1 - Encode Knowledge\n",
        "print(\"--- Task 1: Encoding Knowledge Base ---\")\n",
        "\n",
        "\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base,convert_to_tensor=True)\n",
        "\n",
        "# --- Verification ---\n",
        "if 'knowledge_embeddings' in locals() and knowledge_embeddings.shape[0] == len(knowledge_base):\n",
        "    print(\"âœ… Success! Knowledge base has been encoded.\")\n",
        "    print(f\"   -> Embedding shape: {knowledge_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"âš ï¸ Task 1 not complete. 'knowledge_embeddings' not found or has wrong shape.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsyFiov3pinq",
        "outputId": "af2e4f63-4ff4-49f3-bddc-8823c21dea03"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 1: Encoding Knowledge Base ---\n",
            "âœ… Success! Knowledge base has been encoded.\n",
            "   -> Embedding shape: torch.Size([5, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### âœï¸ Task 2: Build the Retriever Function\n",
        "\n",
        "Now, let's build a function that performs the \"R\" (Retrieval) step. This function will take a user's `query` and find the most relevant document from our `knowledge_base`.\n",
        "\n",
        "**Your Goal:** Complete the `retrieve_context` function.\n",
        "1.  Encode the incoming `query` using the `retriever_model`.\n",
        "2.  Use `util.pytorch_cos_sim` to compare the `query_embedding` to all `knowledge_embeddings`.\n",
        "3.  Find the index of the highest-scoring document (use `torch.argmax`).\n",
        "4.  Return the *text* of that document from the `knowledge_base`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nJLvCx32pnX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 5: Task 2 - Build the Retriever\n",
        "print(\"--- Task 2: Building the Retriever ---\")\n",
        "\n",
        "def retrieve_context(query):\n",
        "    # 1. Encode the query\n",
        "\n",
        "\n",
        "    query_embedding = retriever_model.encode(query,convert_to_tensor=True)\n",
        "    # 2. Compute cosine similarity\n",
        "\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, knowledge_embeddings)[0]\n",
        "\n",
        "\n",
        "\n",
        "    # 3. Find the best match\n",
        "\n",
        "    top_result_index = torch.argmax(cos_scores)\n",
        "\n",
        "    # 4. Return the matching document text\n",
        "\n",
        "    return knowledge_base[top_result_index]\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing retrieve_context('What is Python?')...\")\n",
        "retrieved = retrieve_context(\"What is Python?\")\n",
        "print(f\"   -> Retrieved: '{retrieved}'\")\n",
        "if \"Python\" in retrieved:\n",
        "    print(\"âœ… Success! Retriever function works.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Retriever function failed to find the right document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZF5eaEQpoPU",
        "outputId": "24476770-81f2-4fbf-8f51-c24b215cdf1e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 2: Building the Retriever ---\n",
            "Testing retrieve_context('What is Python?')...\n",
            "   -> Retrieved: 'Python is an interpreted, high-level, general-purpose programming language.'\n",
            "âœ… Success! Retriever function works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### âœï¸ Task 3: Build the Generator Function\n",
        "\n",
        "Great! We have a function to get context. Now let's build a function for the \"G\" (Generation) step. This function will take a `question` and the `context` we just retrieved.\n",
        "\n",
        "**Your Goal:** Complete the `generate_answer` function.\n",
        "1.  Call the `generator_model` (which is a `pipeline` object).\n",
        "2.  Pass the `question` and `context` to it.\n",
        "3.  Return *only the answer* from the resulting dictionary (e.g., `result['answer']`).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ejGGnWtQprE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 6: Task 3 - Build the Generator\n",
        "print(\"\\n--- Task 3: Building the Generator ---\")\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    # 1. Call the pipeline\n",
        "\n",
        "    result = generator_model(question=question, context=context)\n",
        "\n",
        "    # 2. Return the answer\n",
        "\n",
        "    return result['answer']\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing generate_answer('What is Python?', '...')...\")\n",
        "test_context = \"Python is a popular programming language.\"\n",
        "test_question = \"What is Python?\"\n",
        "answer = generate_answer(test_question, test_context)\n",
        "print(f\"   -> Question: '{test_question}'\")\n",
        "print(f\"   -> Context: '{test_context}'\")\n",
        "print(f\"   -> Answer: '{answer}'\")\n",
        "\n",
        "if \"popular programming language\" in answer:\n",
        "    print(\"âœ… Success! Generator function works.\")\n",
        "else:\n",
        "    print(\"âš ï¸ Generator function failed to extract the answer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWeHgxZcpuN2",
        "outputId": "00c97184-92c7-49cb-a4f1-50b3063d3295"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Task 3: Building the Generator ---\n",
            "Testing generate_answer('What is Python?', '...')...\n",
            "   -> Question: 'What is Python?'\n",
            "   -> Context: 'Python is a popular programming language.'\n",
            "   -> Answer: 'a popular programming language'\n",
            "âœ… Success! Generator function works.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### ğŸš€ Task 4: Build the Full RAG Pipeline!\n",
        "\n",
        "This is the final step. Let's combine our two functions into a single, end-to-end RAG pipeline. This function will orchestrate the entire process.\n",
        "\n",
        "**Your Goal:** Complete the `ask_rag_pipeline` function.\n",
        "1.  Call your `retrieve_context` function to get the `best_context` for the `query`.\n",
        "2.  Call your `generate_answer` function, passing in the *original* `query` and the `best_context` you just found.\n",
        "3.  Return the final `answer`.\n",
        "\n",
        "After you write the function, we'll test it with a query!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WXuKM7bHpyM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 7: Task 4 - Build the Full RAG Pipeline\n",
        "print(\"\\n--- Task 4: Building the Full RAG Pipeline ---\")\n",
        "\n",
        "def ask_rag_pipeline(query):\n",
        "    # 1. Retrieve\n",
        "\n",
        "    best_context = retrieve_context(query)\n",
        "\n",
        "    # 2. Generate\n",
        "\n",
        "    final_answer = generate_answer(question=query, context=best_context)\n",
        "\n",
        "    # 3. Return\n",
        "    return final_answer, best_context\n",
        "\n",
        "\n",
        "# --- Verification ---\n",
        "print(\"Testing the full RAG pipeline...\")\n",
        "print(\"Query: 'What is the capital of France?'\")\n",
        "final_answer, retrieved_context = ask_rag_pipeline(\"What is the capital of France?\")\n",
        "\n",
        "print(f\"   -> Retrieved Context: '{retrieved_context}'\")\n",
        "print(f\"   -> Final Answer: '{final_answer}'\")\n",
        "\n",
        "if final_answer.lower() == \"paris\":\n",
        "    print(\"âœ… Success! Your RAG pipeline is working!\")\n",
        "else:\n",
        "    print(\"âš ï¸ RAG pipeline failed. Expected 'Paris'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHtQ-pWlpzga",
        "outputId": "1328e01f-dc71-40eb-ace4-72a22668efb2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Task 4: Building the Full RAG Pipeline ---\n",
            "Testing the full RAG pipeline...\n",
            "Query: 'What is the capital of France?'\n",
            "   -> Retrieved Context: 'The capital of France is Paris, which is known for the Eiffel Tower.'\n",
            "   -> Final Answer: 'Paris'\n",
            "âœ… Success! Your RAG pipeline is working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ğŸ§ª Self-Assessment\n",
        "\n",
        "Run this final cell to test your complete RAG pipeline. This assessment will:\n",
        "1.  Add new information to the agent's knowledge base.\n",
        "2.  Ask questions that *require* the RAG pipeline to work.\n",
        "3.  It will check if your `retrieve_context` function finds the right document AND if your `generate_answer` function extracts the correct answer.\n",
        "\n",
        "Good luck!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KmDt8jCyp32R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Code Cell 8: Self-Assessment\n",
        "print(\"\\n--- ğŸ§ª Self-Assessment ---\")\n",
        "\n",
        "# We will add new documents to the knowledge base and test the agent.\n",
        "# This simulates expanding the agent's memory.\n",
        "\n",
        "try:\n",
        "    # --- Setup for Test ---\n",
        "    new_knowledge = [\n",
        "    \"The Great Wall of China stretches over 13,000 miles and was built to protect against invasions.\",\n",
        "    \"Water boils at 100 degrees Celsius at standard atmospheric pressure.\",\n",
        "    \"Albert Einstein developed the theory of relativity, one of the two pillars of modern physics.\",\n",
        "    \"The Amazon Rainforest is the largest tropical rainforest in the world, spanning several South American countries.\",\n",
        "    \"Mount Everest, located in the Himalayas, is the tallest mountain on Earth above sea level.\",\n",
        "    \"Shakespeare wrote many famous plays including Hamlet, Macbeth, and Romeo and Juliet.\",\n",
        "    \"The Pacific Ocean is the largest and deepest ocean on Earth.\",\n",
        "    \"Electricity is the flow of electric charge, typically carried by electrons through a conductor.\",\n",
        "    \"The human heart has four chambers: two atria and two ventricles.\",\n",
        "    \"Light travels at approximately 299,792 kilometers per second in a vacuum.\",\n",
        "    \"The Nile River is the longest river in Africa, flowing through multiple countries before reaching the Mediterranean Sea.\",\n",
        "    \"Venus is the second planet from the Sun and has a very thick atmosphere composed mostly of carbon dioxide.\",\n",
        "    \"Vaccines stimulate the immune system to provide protection against infectious diseases.\",\n",
        "    \"The first programmable computer was developed by Charles Babbage in the 19th century.\",\n",
        "    \"The Mona Lisa is a famous painting created by Leonardo da Vinci and is displayed in the Louvre Museum.\",\n",
        "    \"Dinosaurs were a diverse group of reptiles that lived millions of years ago and are now extinct.\",\n",
        "    \"Mount Fuji is the highest volcano in Japan and a popular tourist destination.\",\n",
        "    \"The Sahara Desert is the largest hot desert in the world, located in northern Africa.\",\n",
        "    \"Photosynthesis is the process by which plants convert sunlight into chemical energy.\",\n",
        "    \"The Statue of Liberty, located in New York Harbor, was a gift from France to the United States.\"\n",
        "    ]\n",
        "    # Update all the pieces:\n",
        "    # 1. Update the text list\n",
        "    knowledge_base.extend(new_knowledge)\n",
        "    # 2. Update the embeddings (re-encode everything)\n",
        "    knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "    print(f\"ğŸ“š Agent memory updated. Total documents: {len(knowledge_base)}\")\n",
        "\n",
        "    # --- Test Cases ---\n",
        "    extra_test_cases = [\n",
        "    {\n",
        "        \"query\": \"Who developed the theory of relativity?\",\n",
        "        \"expected_context_keyword\": \"Einstein\",\n",
        "        \"expected_answer_keyword\": \"Albert Einstein\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Which river is the longest in Africa?\",\n",
        "        \"expected_context_keyword\": \"Nile River\",\n",
        "        \"expected_answer_keyword\": \"Nile River\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the tallest mountain on Earth?\",\n",
        "        \"expected_context_keyword\": \"Mount Everest\",\n",
        "        \"expected_answer_keyword\": \"Mount Everest\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Which ocean is the largest on Earth?\",\n",
        "        \"expected_context_keyword\": \"Pacific Ocean\",\n",
        "        \"expected_answer_keyword\": \"Pacific Ocean\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the process by which plants convert sunlight into energy?\",\n",
        "        \"expected_context_keyword\": \"Photosynthesis\",\n",
        "        \"expected_answer_keyword\": \"Photosynthesis\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Who painted the Mona Lisa?\",\n",
        "        \"expected_context_keyword\": \"Mona Lisa\",\n",
        "        \"expected_answer_keyword\": \"Leonardo da Vinci\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Which desert is the largest hot desert in the world?\",\n",
        "        \"expected_context_keyword\": \"Sahara Desert\",\n",
        "        \"expected_answer_keyword\": \"Sahara Desert\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What is the speed of light in a vacuum?\",\n",
        "        \"expected_context_keyword\": \"Light\",\n",
        "        \"expected_answer_keyword\": \"299,792 kilometers per second\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Which planet is second from the Sun and has a thick atmosphere?\",\n",
        "        \"expected_context_keyword\": \"Venus\",\n",
        "        \"expected_answer_keyword\": \"Venus\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "    score = 0\n",
        "    total = len(extra_test_cases) * 2\n",
        "    for i, test in enumerate(extra_test_cases):\n",
        "\n",
        "        print(f\"\\n--- Test Case {i+1} ---\")\n",
        "        query = test[\"query\"]\n",
        "        print(f\"Query: \\\"{query}\\\"\")\n",
        "\n",
        "        # Run the full pipeline\n",
        "        answer, context = ask_rag_pipeline(query)\n",
        "\n",
        "        # Check Retrieval\n",
        "        print(f\"   -> ğŸ” Retrieved: '{context}'\")\n",
        "        if test[\"expected_context_keyword\"] in context:\n",
        "            print(\"   -> âœ… Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"   -> âŒ Retrieval Failed. Expected context with: '{test['expected_context_keyword']}'\")\n",
        "\n",
        "        # Check Generation\n",
        "        print(f\"   -> âœï¸ Answer: '{answer}'\")\n",
        "        if test[\"expected_answer_keyword\"].lower() in answer.lower():\n",
        "            print(\"   -> âœ… Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"   -> âŒ Generation Failed. Expected answer with: '{test['expected_answer_keyword']}'\")\n",
        "\n",
        "    # Final Score\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Your Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ‰ğŸ‰ğŸ‰ Perfect! You have successfully built and tested a full RAG pipeline!\")\n",
        "    elif score >= total // 2:\n",
        "        print(\"ğŸ‘ Great job! Your pipeline is working. Review any failed tests to see what happened.\")\n",
        "    else:\n",
        "        print(\"ğŸ”§ Keep trying! Check your functions in Tasks 2, 3, and 4.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- âš ï¸ Assessment Failed ---\")\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please check your code in all tasks and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwKVfPYYp7Pd",
        "outputId": "1a4d73db-7bde-4c3b-a67f-c4ce940b1fb9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ğŸ§ª Self-Assessment ---\n",
            "ğŸ“š Agent memory updated. Total documents: 25\n",
            "\n",
            "--- Test Case 1 ---\n",
            "Query: \"Who developed the theory of relativity?\"\n",
            "   -> ğŸ” Retrieved: 'Albert Einstein developed the theory of relativity, one of the two pillars of modern physics.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Albert Einstein'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 2 ---\n",
            "Query: \"Which river is the longest in Africa?\"\n",
            "   -> ğŸ” Retrieved: 'The Nile River is the longest river in Africa, flowing through multiple countries before reaching the Mediterranean Sea.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'The Nile River'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 3 ---\n",
            "Query: \"What is the tallest mountain on Earth?\"\n",
            "   -> ğŸ” Retrieved: 'Mount Everest, located in the Himalayas, is the tallest mountain on Earth above sea level.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Mount Everest'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 4 ---\n",
            "Query: \"Which ocean is the largest on Earth?\"\n",
            "   -> ğŸ” Retrieved: 'The Pacific Ocean is the largest and deepest ocean on Earth.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Pacific Ocean'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 5 ---\n",
            "Query: \"What is the process by which plants convert sunlight into energy?\"\n",
            "   -> ğŸ” Retrieved: 'Photosynthesis is the process by which plants convert sunlight into chemical energy.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Photosynthesis'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 6 ---\n",
            "Query: \"Who painted the Mona Lisa?\"\n",
            "   -> ğŸ” Retrieved: 'The Mona Lisa is a famous painting created by Leonardo da Vinci and is displayed in the Louvre Museum.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Leonardo da Vinci'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 7 ---\n",
            "Query: \"Which desert is the largest hot desert in the world?\"\n",
            "   -> ğŸ” Retrieved: 'The Sahara Desert is the largest hot desert in the world, located in northern Africa.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Sahara Desert'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 8 ---\n",
            "Query: \"What is the speed of light in a vacuum?\"\n",
            "   -> ğŸ” Retrieved: 'Light travels at approximately 299,792 kilometers per second in a vacuum.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: '299,792 kilometers per second'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- Test Case 9 ---\n",
            "Query: \"Which planet is second from the Sun and has a thick atmosphere?\"\n",
            "   -> ğŸ” Retrieved: 'Venus is the second planet from the Sun and has a very thick atmosphere composed mostly of carbon dioxide.'\n",
            "   -> âœ… Retrieval Correct!\n",
            "   -> âœï¸ Answer: 'Venus'\n",
            "   -> âœ… Generation Correct!\n",
            "\n",
            "--- ğŸ Assessment Complete ---\n",
            "ğŸ¯ Your Final Score: 18 / 18\n",
            "ğŸ‰ğŸ‰ğŸ‰ Perfect! You have successfully built and tested a full RAG pipeline!\n"
          ]
        }
      ]
    }
  ]
}
